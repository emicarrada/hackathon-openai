# ğŸ¯ ISRAEL - TAREAS ESPECÃFICAS HACKATHON

## ğŸ“‹ TU MISIÃ“N

**Rol:** Experto en Prompts y OptimizaciÃ³n de IA  
**Objetivo:** Mejorar prompts del sistema y agregar tipos de tarea  
**Tiempo estimado:** 2 horas  
**Rama de trabajo:** `israel/prompts-optimizacion`

---

## âœ… ANTES DE EMPEZAR

### 1. Rebase con main

```bash
# Guardar tu trabajo actual
git stash

# Actualizar desde main
git checkout main
git pull origin main

# Crear tu rama de trabajo
git checkout -b israel/prompts-optimizacion

# Recuperar trabajo anterior (si lo necesitas)
git stash pop
```

### 2. Verificar que el sistema funciona

```bash
# Configurar API Key
export OPENAI_API_KEY="tu_api_key_aqui"

# Probar el sistema
python demo_hackathon.py --rapida
```

**Output esperado:** Ver Run 1 vs Run 2 funcionando correctamente.

### 3. Leer documentaciÃ³n

- [ ] `GUIA_COMPLETA_6_NODOS.md` - Entender arquitectura completa
- [ ] `src/nodos/recibir_tarea.py` - ClasificaciÃ³n de tareas
- [ ] `src/nodos/auditor_feedback.py` - Prompts del auditor
- [ ] `src/nodos/ejecutar_tarea.py` - EjecuciÃ³n de tareas

---

## ğŸ¯ TAREAS PRIORITARIAS (EN ORDEN)

### TAREA 1: Mejorar ClasificaciÃ³n de Tareas (45 min)

**Archivo:** `src/nodos/recibir_tarea.py`

#### Estado Actual

```python
# ClasificaciÃ³n bÃ¡sica por keywords
if any(word in tarea_lower for word in ["resume", "resumen", "resumir"]):
    tipo_tarea = "resumen"
elif any(word in tarea_lower for word in ["traduce", "traducir", "translate"]):
    tipo_tarea = "traduccion"
# ... solo 5 tipos
```

#### Lo que DEBES Agregar

**1. MÃ¡s tipos de tarea:**

```python
TIPOS_TAREA = {
    "resumen": ["resume", "resumen", "resumir", "sintetiza", "sinopsis"],
    "traduccion": ["traduce", "traducir", "translate", "translation"],
    "clasificacion": ["clasifica", "categoriza", "classify", "category"],
    "extraccion": ["extrae", "extract", "obtÃ©n", "encuentra"],
    "analisis": ["analiza", "analyze", "evalÃºa", "assess"],  # NUEVO
    "codigo": ["codigo", "code", "programa", "script", "debug"],  # NUEVO
    "creatividad": ["escribe", "crea", "genera", "inventa", "historia"],  # NUEVO
    "qa": ["pregunta", "responde", "question", "answer", "quÃ© es"],  # NUEVO
    "comparacion": ["compara", "compare", "diferencias", "vs"],  # NUEVO
    "general": []  # Fallback
}
```

**2. ClasificaciÃ³n mÃ¡s inteligente:**

En lugar de solo keywords, usar un mini-LLM para clasificar (opcional pero impresionante):

```python
def clasificar_con_llm(tarea: str) -> str:
    """
    Usa GPT-4o-mini (barato) para clasificar tarea con alta precisiÃ³n.
    """
    prompt = f"""Clasifica esta tarea en UNA de estas categorÃ­as:
- resumen
- traduccion
- clasificacion
- extraccion
- analisis
- codigo
- creatividad
- qa
- comparacion
- general

Tarea: "{tarea}"

Responde SOLO con la categorÃ­a (una palabra, minÃºsculas):"""
    
    # Llamar a OpenAI con gpt-4o-mini
    # ...
    return categoria
```

#### Prompt para Copilot

```
Mejora src/nodos/recibir_tarea.py para:

1. Agregar estos nuevos tipos de tarea:
   - analisis: para anÃ¡lisis profundos
   - codigo: para generaciÃ³n/debugging de cÃ³digo
   - creatividad: para escritura creativa
   - qa: para preguntas y respuestas
   - comparacion: para comparar conceptos

2. Ampliar las keywords para cada tipo (mÃ­nimo 5 por tipo)

3. OPCIONAL: Agregar funciÃ³n clasificar_con_llm() que:
   - Use gpt-4o-mini para clasificar con IA
   - Solo se active si keywords no dan match claro
   - Tenga fallback a clasificaciÃ³n por keywords
   - Sea eficiente (mÃ¡ximo 50 tokens)

4. Agregar logging para debug:
   print(f"ğŸ” Clasificando: {tarea[:50]}...")
   print(f"âœ“ Tipo detectado: {tipo_tarea}")

5. Mantener compatibilidad con cÃ³digo existente

Incluye docstrings, type hints y manejo de errores.
```

#### Resultado Esperado

```python
# Tareas que antes eran "general" ahora se clasifican correctamente
"Genera un script Python" â†’ "codigo"
"Compara Python vs JavaScript" â†’ "comparacion"
"Analiza este dataset" â†’ "analisis"
"Escribe un poema" â†’ "creatividad"
```

---

### TAREA 2: Optimizar Prompt del Auditor (45 min)

**Archivo:** `src/nodos/auditor_feedback.py`

#### Estado Actual

```python
prompt_auditor = f"""Eres un Auditor de Eficiencia de APIs de IA.

Analiza esta ejecuciÃ³n:
- Tipo de tarea: {tipo_tarea}
- Modelo usado: {modelo_usado}
- Tokens consumidos: {tokens_totales}

Contexto de costos:
- GPT-4o: modelo mÃ¡s caro (~10x GPT-3.5-turbo)
- GPT-3.5-turbo: modelo econÃ³mico, buen rendimiento
- GPT-4o-mini: modelo ultra-econÃ³mico

EvalÃºa si el modelo usado fue eficiente para esta tarea.
..."""
```

#### Lo que DEBES Mejorar

**1. Agregar ejemplos de referencia por tipo de tarea:**

```python
BENCHMARKS_POR_TIPO = {
    "resumen": {
        "modelo_optimo": "gpt-3.5-turbo",
        "tokens_esperados": 200,
        "razon": "Tareas de resumen no requieren razonamiento complejo"
    },
    "codigo": {
        "modelo_optimo": "gpt-4o",
        "tokens_esperados": 500,
        "razon": "GeneraciÃ³n de cÃ³digo requiere precisiÃ³n"
    },
    "analisis": {
        "modelo_optimo": "gpt-4o",
        "tokens_esperados": 800,
        "razon": "AnÃ¡lisis profundo requiere capacidad de razonamiento"
    },
    # ... resto de tipos
}
```

**2. Prompt mÃ¡s especÃ­fico:**

```python
prompt_auditor = f"""Eres un Auditor Senior de Eficiencia en APIs de IA.

CONTEXTO DE LA EJECUCIÃ“N:
- Tipo de tarea: {tipo_tarea}
- Modelo usado: {modelo_usado}
- Tokens consumidos: {tokens_totales}
- Costo estimado: ${costo_estimado}

BENCHMARK PARA "{tipo_tarea}":
- Modelo Ã³ptimo: {benchmark["modelo_optimo"]}
- Tokens esperados: {benchmark["tokens_esperados"]}
- RazÃ³n: {benchmark["razon"]}

TABLA DE COSTOS (por 1M tokens):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Modelo       â”‚ Input  â”‚ Output  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ GPT-4o       â”‚ $2.50  â”‚ $10.00  â”‚
â”‚ GPT-3.5      â”‚ $0.50  â”‚ $1.50   â”‚
â”‚ GPT-4o-mini  â”‚ $0.15  â”‚ $0.60   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INSTRUCCIONES:
1. Compara el modelo usado con el benchmark
2. Analiza si hubo desperdicio de recursos
3. Considera calidad vs costo
4. Responde en JSON exacto:

{{
    "requiere_optimizacion": true/false,
    "analisis": "explicaciÃ³n tÃ©cnica en 1-2 lÃ­neas",
    "recomendacion": "modelo_recomendado" o "ninguna"
}}

IMPORTANTE:
- Si modelo usado == modelo Ã³ptimo â†’ requiere_optimizacion: false
- Si usaste GPT-4o para tarea simple â†’ requiere_optimizacion: true
- Si usaste GPT-3.5 para tarea compleja â†’ revisar calidad antes
"""
```

#### Prompt para Copilot

```
Mejora src/nodos/auditor_feedback.py para:

1. Crear diccionario BENCHMARKS_POR_TIPO con:
   - Modelo Ã³ptimo por tipo de tarea
   - Tokens esperados
   - RazÃ³n tÃ©cnica

2. Mejorar prompt del auditor para que:
   - Compare con benchmark especÃ­fico
   - Muestre tabla de costos clara
   - DÃ© anÃ¡lisis mÃ¡s tÃ©cnico y preciso
   - Use el benchmark en su evaluaciÃ³n

3. Agregar lÃ³gica de "zona gris":
   - Si diferencia de costo < 20% â†’ no optimizar (no vale la pena)
   - Si tokens > 2x esperados â†’ investigar por quÃ©

4. Mejor manejo de JSON:
   - Validar que todos los campos existan
   - Defaults sensatos si parsing falla
   - Logging de errores

5. Agregar comentarios explicativos

MantÃ©n compatibilidad con el resto del sistema.
```

#### Resultado Esperado

El auditor ahora da feedback mÃ¡s especÃ­fico:

```
ANTES:
"El uso de GPT-4o para esta tarea no es eficiente."

DESPUÃ‰S:
"Tarea 'resumen' detectada. Benchmark: gpt-3.5-turbo (200 tokens).
Usaste gpt-4o (128 tokens) = 10x mÃ¡s caro sin beneficio de calidad.
RecomendaciÃ³n: Cambiar a gpt-3.5-turbo para ahorrar 90% en costos."
```

---

### TAREA 3: Mejorar EjecuciÃ³n de Tareas (30 min)

**Archivo:** `src/nodos/ejecutar_tarea.py`

#### Estado Actual

```python
# Prompt muy genÃ©rico
mensaje_sistema = "Eres un asistente Ãºtil."
```

#### Lo que DEBES Mejorar

**Prompts especializados por tipo de tarea:**

```python
PROMPTS_SISTEMA = {
    "resumen": """Eres un experto en sÃ­ntesis de informaciÃ³n.
Tu objetivo: resumir contenido de forma clara y concisa.
Estilo: Puntos clave, mÃ¡ximo detalle en mÃ­nimas palabras.""",
    
    "codigo": """Eres un programador senior experto.
Tu objetivo: generar cÃ³digo limpio, eficiente y bien documentado.
Estilo: Best practices, type hints, comentarios claros.""",
    
    "analisis": """Eres un analista senior con pensamiento crÃ­tico.
Tu objetivo: anÃ¡lisis profundo con insights accionables.
Estilo: Estructurado, basado en datos, conclusiones claras.""",
    
    "creatividad": """Eres un escritor creativo profesional.
Tu objetivo: contenido original, engaging y bien escrito.
Estilo: Narrativo, emotivo, con buen ritmo.""",
    
    # ... resto de tipos
}
```

#### Prompt para Copilot

```
Mejora src/nodos/ejecutar_tarea.py para:

1. Crear diccionario PROMPTS_SISTEMA con pe llamerompts especializados para:
   - resumen
   - traduccion
   - codigo
   - analisis
   - creatividad
   - qa
   - comparacion
   - clasificacion
   - extraccion
   - general (fallback)

2. Seleccionar prompt segÃºn tipo_tarea del estado

3. Agregar parÃ¡metros Ã³ptimos por tipo:
   - resumen: temperature=0.3 (mÃ¡s determinÃ­stico)
   - creatividad: temperature=0.9 (mÃ¡s creativo)
   - codigo: temperature=0.2 (muy determinÃ­stico)
   - general: temperature=0.7 (balanceado)

4. Mejorar mensajes de usuario:
   - MÃ¡s contexto
   - Instrucciones claras
   - Formato de salida esperado

5. Mantener max_tokens y estructura actual

Documenta por quÃ© cada prompt es Ã³ptimo para su tipo.
```

#### Resultado Esperado

```python
# Antes: mismo prompt genÃ©rico para todo
# DespuÃ©s: prompt optimizado por tipo

# Para resumen:
sistema = "Eres un experto en sÃ­ntesis..."
temperature = 0.3

# Para creatividad:
sistema = "Eres un escritor creativo..."
temperature = 0.9
```

---

## ğŸ§ª TESTING

Crear `tests/test_prompts.py`:

```python
def test_clasificacion_extendida():
    """Verifica nuevos tipos de tarea"""
    from src.nodos.recibir_tarea import recibir_tarea
    
    state = {"tarea_descripcion": "Escribe un poema sobre IA"}
    resultado = recibir_tarea(state)
    assert resultado["tipo_tarea"] == "creatividad"
    
    state = {"tarea_descripcion": "Genera un script Python"}
    resultado = recibir_tarea(state)
    assert resultado["tipo_tarea"] == "codigo"

def test_benchmarks_auditor():
    """Verifica que benchmarks existen"""
    from src.nodos.auditor_feedback import BENCHMARKS_POR_TIPO
    
    assert "resumen" in BENCHMARKS_POR_TIPO
    assert "codigo" in BENCHMARKS_POR_TIPO
    assert "modelo_optimo" in BENCHMARKS_POR_TIPO["resumen"]

def test_prompts_especializados():
    """Verifica prompts por tipo"""
    from src.nodos.ejecutar_tarea import PROMPTS_SISTEMA
    
    assert len(PROMPTS_SISTEMA) >= 9
    assert "resumen" in PROMPTS_SISTEMA
    assert len(PROMPTS_SISTEMA["resumen"]) > 50  # Prompt decente
```

---

## ğŸ¨ BONUS: Demo de Tipos de Tarea

Crear `demo_tipos_tarea.py` para mostrar a los jueces:

```python
"""
Demo de clasificaciÃ³n inteligente de tareas.
Muestra cÃ³mo el sistema detecta diferentes tipos.
"""

from src.agente import SmartOptimizerAgent

tareas_ejemplo = [
    "Resume este artÃ­culo sobre IA",
    "Traduce al inglÃ©s: Hola mundo",
    "Escribe un poema sobre Python",
    "Genera un script que ordene una lista",
    "Compara Python vs JavaScript",
    "Analiza el impacto de la IA en empleos",
]

agente = SmartOptimizerAgent()

print("ğŸ¯ DEMO: ClasificaciÃ³n Inteligente de Tareas\n")

for i, tarea in enumerate(tareas_ejemplo, 1):
    print(f"{i}. Tarea: {tarea}")
    resultado = agente.ejecutar(tarea)
    tipo = resultado.get("tipo_tarea", "?")
    modelo = resultado.get("metricas_ejecucion", {}).get("modelo_usado", "?")
    print(f"   â†’ Tipo: {tipo}")
    print(f"   â†’ Modelo: {modelo}")
    print()
```

---

## ğŸ“¤ SUBIR TU TRABAJO

```bash
# 1. Verificar cambios
git status

# 2. Agregar archivos
git add src/nodos/recibir_tarea.py
git add src/nodos/auditor_feedback.py
git add src/nodos/ejecutar_tarea.py
git add tests/test_prompts.py
git add demo_tipos_tarea.py  # si lo hiciste

# 3. Commit
git commit -m "Optimizar prompts y clasificaciÃ³n de tareas

- Agregar 5 nuevos tipos de tarea (cÃ³digo, anÃ¡lisis, etc.)
- Mejorar clasificaciÃ³n con mÃ¡s keywords
- Crear benchmarks por tipo de tarea en auditor
- Prompts especializados por tipo en ejecutar_tarea
- Temperature Ã³ptimo por tipo de tarea
- Tests de clasificaciÃ³n y prompts"

# 4. Push a tu rama
git push origin israel/prompts-optimizacion

# 5. Avisar a Carrada en Discord/Slack
```

---

## ğŸ¤ NARRATIVA PARA TU PARTE

Cuando expliques en la presentaciÃ³n:

> **"OptimicÃ© el sistema de prompts para que cada tipo de tarea use el prompt mÃ¡s efectivo. Por ejemplo, para cÃ³digo usamos temperature 0.2 (determinÃ­stico) y prompts de 'programador senior', mientras que para creatividad usamos 0.9 y prompts de 'escritor profesional'. El auditor ahora compara contra benchmarks especÃ­ficos por tipo de tarea, haciendo recomendaciones mucho mÃ¡s precisas."**

---

## ğŸ†˜ SI TIENES PROBLEMAS

### Problema: No sÃ© quÃ© prompts usar

**SoluciÃ³n:**
1. Lee ejemplos en ChatGPT best practices: https://platform.openai.com/docs/guides/prompt-engineering
2. Mira prompts exitosos en `src/nodos/auditor_feedback.py` (ya funciona bien)
3. Pide a Copilot: "Dame 3 variaciones de prompt para [tipo_tarea]"

### Problema: ClasificaciÃ³n falla con algunas tareas

**SoluciÃ³n:**
```python
# Agregar logging para debug
print(f"DEBUG: tarea = {tarea}")
print(f"DEBUG: keywords encontradas = {keywords_match}")
print(f"DEBUG: tipo final = {tipo_tarea}")
```

### Problema: Auditor no da buenas recomendaciones

**SoluciÃ³n:**
1. Verifica que BENCHMARKS_POR_TIPO tenga valores sensatos
2. Prueba manualmente con diferentes tipos
3. Ajusta los benchmarks basÃ¡ndote en resultados reales

---

## â° CHECKLIST FINAL

Antes de hacer push, verifica:

- [ ] `recibir_tarea.py` tiene mÃ­nimo 9 tipos de tarea
- [ ] Cada tipo tiene 5+ keywords
- [ ] `auditor_feedback.py` tiene benchmarks para todos los tipos
- [ ] Prompt del auditor es claro y especÃ­fico
- [ ] `ejecutar_tarea.py` tiene prompts especializados
- [ ] Temperatures configurados por tipo
- [ ] Tests pasan: `pytest tests/test_prompts.py`
- [ ] Demo funciona: `python demo_hackathon.py`
- [ ] CÃ³digo documentado con docstrings

---

## ğŸ† IMPACTO DE TU TRABAJO

Tu implementaciÃ³n permite:
- âœ… **ClasificaciÃ³n inteligente** de 9+ tipos de tarea
- âœ… **Prompts optimizados** por tipo (mejor calidad)
- âœ… **AuditorÃ­a precisa** con benchmarks especÃ­ficos
- âœ… **Sistema mÃ¡s profesional** y robusto

**Â¡Tu parte hace el sistema 10x mÃ¡s inteligente!** ğŸš€

---

## ğŸ“š RECURSOS

- **Prompt Engineering:** https://platform.openai.com/docs/guides/prompt-engineering
- **Best Practices:** https://help.openai.com/en/articles/6654000-best-practices-for-prompt-engineering
- **Temperature Guide:** https://platform.openai.com/docs/guides/text-generation

Â¡Ã‰xito Israel! ğŸ’ª
