# Hackathon OpenAI

## DescripciÃ³n
Proyecto para Hackathon Kavak x OpenAI MÃ©xico 2025: Agente que evalÃºa la complejidad de tareas y selecciona el modelo LLM mÃ¡s eficiente (GPT-3.5 vs GPT-4), refinando respuestas con un solo LLM y validando calidad con un Juez LLM. Demuestra "IA que piensa antes de gastar".

## InstalaciÃ³n

1. Clona el repositorio
2. Instala dependencias: `pip install -r requirements.txt`
3. Configura variables de entorno: `cp env.template .env`

## Estructura del Proyecto

- `src/`: CÃ³digo fuente
- `data/`: Datos persistentes
- `docs/`: DocumentaciÃ³n
- `notebooks/`: Notebooks de desarrollo
- `tests/`: Tests unitarios

## Arquitectura Simplificada (3 Nodos)

- **Nodo 1: EvaluaciÃ³n Contextual** (`src/nodos/evaluar_complejidad.py`): Analiza complejidad de la tarea.
- **Nodo 2: GeneraciÃ³n y Refinamiento** (`src/nodos/generar_refinar.py`): Genera y refina con un solo LLM.
- **Nodo 3: ValidaciÃ³n de Calidad** (`src/nodos/validar_calidad.py`): Valida con LLM-Juez.

## SeparaciÃ³n de Trabajo (Compliant con Reglas 5.4/5.5)

### Preparado Pre-Evento (22 oct)

- âœ… Infraestructura: Cliente OpenAI, mÃ©tricas, cache demo.
- âœ… Arquitectura: Stubs en `src/agente.py`, `src/nodos/`, `src/memoria.py`, `src/juez.py`.
- âœ… DocumentaciÃ³n: Este README y anÃ¡lisis de reglas.
- âœ… Boilerplate: Estructura de proyecto, dependencias.

### Implementado en Evento (23 oct)

- ğŸ”„ LÃ³gica de EvaluaciÃ³n: AnÃ¡lisis de complejidad (longitud, keywords).
- ğŸ”„ GeneraciÃ³n/Refinamiento: Prompts para auto-feedback con un solo LLM.
- ğŸ”„ ValidaciÃ³n: Juez LLM para comparaciÃ³n objetiva.
- ğŸ”„ Demos: Casos mixtos (GPT-3.5 gana en simple, GPT-4 en complejo).
- ğŸ”„ Testing: ValidaciÃ³n end-to-end y mÃ©tricas de ahorro.

**Nota:** Nada de funcionalidad completa pre-evento. Todo preparado es stubs o soporte.

## Estado del Proyecto

Repositorio limpio con solo trabajo compliant. Proyecto rediseÃ±ado para simplicidad y mÃ¡xima chance de ganar.
