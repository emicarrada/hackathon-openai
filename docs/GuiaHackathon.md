# GU√çA PARA PARTICIPANTES

### Hackathon Kavak x OpenAI: Sistemas de AI Auto-Mejorables

```
23 de octubre de 2025 | Ciudad de M√©xico
```
## ¬°Bienvenido!

Has sido seleccionado para participar en nuestro hackathon enfocado en una de las
fronteras m√°s emocionantes de AI: **sistemas que pueden mejorarse a s√≠ mismos**.
Esta gu√≠a te ayudar√° a entender qu√© estamos buscando, c√≥mo prepararte y c√≥mo
maximizar tus posibilidades de √©xito el d√≠a del evento.

## Entendiendo el Reto

### ¬øQu√© Estamos Buscando Realmente?

Los sistemas de AI auto-mejorables no se tratan de superinteligencia exponencial y
aut√≥noma. Estamos hablando de **sistemas pr√°cticos con mecanismos robustos
de evaluaci√≥n** que pueden identificar sus propias debilidades y actuar en base a
ese feedback‚Äîya sea de forma aut√≥noma o semi-autom√°tica.

### Los Componentes Centrales

Un sistema auto-mejorable necesita:

1. **Mecanismo de evaluaci√≥n** - ¬øC√≥mo mide el sistema su propio desempe√±o?
2. **Ciclo de retroalimentaci√≥n** - ¬øC√≥mo identifica qu√© sali√≥ mal o qu√© podr√≠a
    mejorar?
3. **Mecanismo de acci√≥n** - ¬øC√≥mo implementa mejoras basadas en ese
    feedback?
4. **Mejora medible** - ¬øPuedes probar que Run 2 es mejor que Run 1 y
    subsequente?

### Example Project

**Personalized Customer Service AI:**

- **Run 1:** AI agent handles customer query with generic approach
- **Evaluation:** System analyzes conversation quality, customer satisfaction signals, unresolved issues
- **Learning:** Updates customer memory profile, identifies communication patterns that work/don't work, refines orchestration strategy
- **Run 2:** Same customer or similar query ‚Üí AI uses learned patterns, personalized memory, improved orchestration
- **Result:** Measurably better responses (faster resolution, higher satisfaction, fewer escalations)

This works because:


- M√©tricas de evaluaci√≥n claras (tiempo de resoluci√≥n, satisfacci√≥n)
- Mecanismo de feedback concreto (an√°lisis de conversaci√≥n)
- Mejoras accionables (actualizaciones de memoria, refinamiento de prompts,
    cambios en orquestaci√≥n)
- Mejora demostrable que puedes mostrar

### ¬øQu√© hace que los Sistemas se Auto-Mejoren?

Diferentes mecanismos que podr√≠as explorar:
**Ingenier√≠a de Prompts:**

- Sistema analiza outputs fallidos
- Genera prompts mejorados
- Prueba y valida mejoras
**Orquestaci√≥n Multi-Agente:**
- Agentes eval√∫an los outputs de otros
- Coordinador aprende qu√© agentes usar cu√°ndo
- El enrutamiento mejora basado en desempe√±o pasado
**Sistemas de Memoria:**
- Sistema construye base de conocimiento de interacciones
- Recupera contexto relevante para consultas futuras
- Desempe√±o mejora conforme crece la memoria
**Evoluci√≥n de Arquitectura:**
- Sistema identifica cuellos de botella en su propio pipeline
- Propone cambios arquitect√≥nicos
- Implementa y valida mejoras
**Meta-Aprendizaje:**
- Sistema aprende "c√≥mo aprender" de ejemplos
- Identifica patrones en sus errores
- Generaliza estrategias de mejora

### Conceptos Err√≥neos Comunes

‚ùå "Debe ser completamente aut√≥nomo" ‚Üí Semi-automatizado est√° bien. Lo que
importa es que el sistema genere insights y mejoras, incluso si hay validaci√≥n
humana.
‚ùå "M√°s complejo = mejor" ‚Üí Falso. Un ciclo simple y funcional de auto-mejora
vence a uno complejo y roto cada vez.
‚ùå "Necesito t√©cnicas novedosas de ML" ‚Üí No necesitas inventar nuevos
algoritmos. Arquitectura inteligente y evaluaci√≥n clara ganan.
‚ùå "El dominio del problema importa m√°s" ‚Üí La elecci√≥n del problema importa para
puntos de creatividad, pero el mecanismo de auto-mejora es lo que realmente
juzgamos.


## R√∫brica de Evaluaci√≥n

```
Filosof√≠a de Evaluaci√≥n: Este hackathon celebra la creatividad y diversidad de
enfoques. La r√∫brica proporciona un marco estructurado pero flexible que permite a
los jueces evaluar proyectos muy diferentes de manera justa.
```
### Principios Fundamentales de Evaluaci√≥n

```
‚óè Flexibilidad con Rigor: Los proyectos pueden demostrar auto-mejora
de m√∫ltiples maneras v√°lidas
‚óè Contexto es Clave: Una mejora del 5% en un problema dif√≠cil puede
ser m√°s impresionante que 50% en uno trivial
‚óè El Proceso Importa: Un mecanismo inteligente con mejora modesta
vence a resultados no explicados
‚óè Demostraci√≥n > Promesas: Se juzga lo que funciona, no las
intenciones
‚óè Simple y Funcional > Complejo y Roto: La ejecuci√≥n cuenta m√°s
que la ambici√≥n
```
### Expectativas M√≠nimas

```
Estos son elementos esperados que facilitan la evaluaci√≥n, pero su ausencia no
descalifica autom√°ticamente un proyecto:
```
- **Al menos dos iteraciones del sistema**
- **Casos de prueba documentados** (evaluation set de al menos 3-5 casos;
    m√°s es mejor)
- **Alguna forma de medici√≥n** (m√©tricas cuantitativas preferidas)
- **Evidencia de aprendizaje** (alg√∫n mecanismo que muestre que las versiones
    posteriores incorpora conocimiento de la primera)
_Nota importante: Si un proyecto tiene una innovaci√≥n excepcional en su enfoque de
auto-mejora pero le falta uno de estos elementos, los jueces pueden usar
discreci√≥n. Lo cr√≠tico es demostrar mejora convincente de alguna forma._

### Distribuci√≥n de Puntos (Total: 100)

```
Categor√≠a Puntos Enfoque
```
**1. Demostraci√≥n de Auto-Mejora** 35 Evidencia + Mecanismo
**2. Funcionalidad y Ejecuci√≥n** 25 ¬øFunciona?
**3. Creatividad e Innovaci√≥n** 25 Enfoque + Problema
**4. Presentaci√≥n y Claridad** 15 Comunicaci√≥n
**TOTAL 100**


### 1. Demostraci√≥n de Auto-Mejora (35 puntos)

#### A. Evidencia de Mejora (20 puntos)

_¬øEl sistema demuestra una mejora clara y medible?_
**Puntos Nivel Descripci√≥n
18-
Excepcional** Mejora dram√°tica que transforma la capacidad del sistema. Corridas
posteriores resuelven que no pod√≠a manejar previamente.
**14-
Fuerte** Mejora clara y sustancial en m√©tricas clave. El sistema es notablemente m√°s
efectivo en Run 2.
**9-
S√≥lida** Mejora medible y consistente. Run 2 supera a Run 1 de manera observable
y repetible.
**4-8 Marginal** Mejora peque√±a o inconsistente. Algunos casos mejoran, otros no.
**0-3 Insuficiente** Sin mejora clara, resultados ambiguos, o Run 2 es peor que Run 1.
**Consideraciones del juez:**

- Dificultad inherente del problema elegido
- Consistencia de mejora a trav√©s de los casos de prueba
- Claridad de las m√©tricas

#### B. Sofisticaci√≥n del Mecanismo (15 puntos)

```
¬øQu√© tan inteligente y robusto es el proceso de auto-mejora?
Puntos Nivel Descripci√≥n
13-
Innovador Mecanismo novedoso. El sistema analiza fallos profundamente,
identifica patrones de error espec√≠ficos, y genera mejoras dirigidas.
Altamente automatizado.
10-
Sofisticado Feedback loop bien dise√±ado. Identifica QU√â fall√≥ ‚Üí POR QU√â fall√≥
‚Üí C√ìMO corregirlo. Buena automatizaci√≥n.
6-9 Funcional^ Feedback^ loop^ b√°sico^ pero^ efectivo.^ Detecta^ errores^ y^ hace^ ajustes^
razonables. Puede requerir intervenci√≥n humana moderada.
3-
Rudimentario Mejora principalmente manual o basada en prueba-y-error simple.
Poca automatizaci√≥n.
0-
Inexistente No hay mecanismo real de auto-mejora. Cambios aleatorios o
modificaciones manuales sin sistema.
El juez eval√∫a:
```
- Nivel de automatizaci√≥n (completamente autom√°tico > semi-autom√°tico >
    principalmente manual)
- Profundidad del an√°lisis de errores (¬øentiende POR QU√â fall√≥, no solo QUE
    fall√≥?)
- Capacidad de generalizar aprendizajes a nuevos casos
- Robustez del mecanismo en diferentes escenarios


### 2. Funcionalidad y Ejecuci√≥n (25 puntos)

```
¬øEl sistema funciona de punta a punta de manera confiable?
Puntos Nivel Descripci√≥n
22-25 Excepcional
Demo sin errores, el ciclo de auto-mejora funciona completamente
end-to-end, resultados claros y verificables con m√©tricas bien
documentadas.
16-21 S√≥lido
Funciona con bugs menores, ciclo de mejora completo o casi completo,
resultados visibles y comprensibles.
10-15 Funcional
Funcionalidad b√°sica, algunos componentes del ciclo funcionan, resultados
parciales pero demostrables.
0-9 Limitado
Problemas significativos, ciclo incompleto, resultados poco claros o demo no
funciona.
```
### 3. Creatividad e Innovaci√≥n (25 puntos)

#### A. Originalidad del Enfoque T√©cnico (15 puntos)

```
¬øEs el approach t√©cnico √∫nico, creativo, o innovador?
Puntos Nivel Descripci√≥n
13-
Innovador T√©cnica o combinaci√≥n novedosa. Pensamiento lateral que sorprende.
Approach √∫nico que no hab√≠amos considerado.
10-
Creativo Twist inteligente en t√©cnicas conocidas. Uso no convencional de
herramientas. Combinaci√≥n interesante de m√©todos.
6-
Competente Implementaci√≥n s√≥lida de t√©cnicas familiares. Bien ejecutado pero no
innovador. Approach esperado.
0-5 Gen√©rico^ Soluci√≥n^ est√°ndar^ sin^ elementos^ distintivos.^ Lo^ que^ cualquiera^ habr√≠a^
intentado.
```
#### B. Elecci√≥n e Interpretaci√≥n del Problema (10 puntos)

_¬øEs interesante el problema elegido o su planteamiento?_
**Puntos Nivel Descripci√≥n
9-
Inspirador** Dominio inesperado o √°ngulo totalmente fresco en problema conocido. Muestra
entendimiento profundo del dominio.
**7-
S√≥lido** Problema relevante bien planteado. Buen caso de uso para auto-mejora.
Aplicaci√≥n clara.
**4-
Est√°ndar** Problema t√≠pico de hackathon (chatbot, code gen, Q&A). Funciona pero no
innova en la elecci√≥n.
**0-3 Poco claro** Problema confuso, trivial, o mal definido. No se entiende por qu√© eligieron esto.


### 4. Presentaci√≥n y Claridad (15 puntos)

_¬øPodemos entender qu√© hicieron y por qu√© importa?_
**Puntos Nivel Descripci√≥n
13-15 Excepcional**
Problema cristalino en <1 min, la mejora es obvia y medible, explicaci√≥n
l√≥gica del mecanismo, documentaci√≥n impecable.
**10-12 S√≥lido**
Problema claro, mejora iterativa visible, explicaci√≥n comprensible del
mecanismo, documentaci√≥n estructurada.
**6-9 Adecuado** Se^ entiende^ el^ problema,^ mejora^ demostrable,^ explicaci√≥n^ b√°sica^ del^
proceso, documentaci√≥n presente.
**0-5 Confuso**
Problema poco claro, mejora dif√≠cil de ver, explicaci√≥n confusa,
documentaci√≥n escasa o ausente.
**Importa:** Claridad, evidencia, storytelling simple, documentaci√≥n estructurada
**No importa:** Slides bonitos, UI pulido, marketing speak

## Frameworks de Evaluaci√≥n

```
Para ayudarte a dise√±ar sistemas auto-mejorables robustos, aqu√≠ presentamos
frameworks de evaluaci√≥n establecidos que puedes aplicar:
```
### 1. Framework de Evaluaci√≥n de Agentes de AI

```
Dimensiones de Evaluaci√≥n:
```
- **Razonamiento y Planificaci√≥n:** ¬øPuede el sistema descomponer tareas
    complejas?
- **Uso de Herramientas:** ¬øSelecciona y usa las herramientas correctas?
- **Auto-Reflexi√≥n:** ¬øIdentifica y corrige sus propios errores?
- **Memoria y Contexto:** ¬øRetiene informaci√≥n relevante entre interacciones?
- **Generalizaci√≥n:** ¬øFunciona en casos no vistos?

### 2. M√©tricas Cuantitativas Comunes

```
Para Sistemas de Q&A o RAG:
```
- Precisi√≥n de respuestas (accuracy)
- Relevancia de documentos recuperados (NDCG, MRR)
- Completitud de respuestas
- Fidelidad (responde basado en fuentes, sin alucinar)
**Para Sistemas de C√≥digo:**
- Pass@k (porcentaje de soluciones correctas)
- Cobertura de tests
- Eficiencia del c√≥digo generado
- Errores de compilaci√≥n/ejecuci√≥n
**Para Sistemas Conversacionales:**


- Tasa de resoluci√≥n de problemas
- Tiempo promedio de resoluci√≥n
- Satisfacci√≥n del usuario (si aplicable)
- N√∫mero de turnos conversacionales

### 3. Framework de Evaluaci√≥n Cualitativa

Cuando las m√©tricas cuantitativas son dif√≠ciles, usa evaluaci√≥n estructurada:

- **Escala Likert:** 1-5 o 1-7 en dimensiones espec√≠ficas
- **Comparaci√≥n Pareada:** ¬øOutput A o B es mejor?
- **An√°lisis de Errores:** Clasificar tipos de fallos
- **LLM-as-Judge:** Usar otro LLM para evaluar outputs

### 4. Herramientas de Evaluaci√≥n Recomendadas

- **LangSmith** - Plataforma de observabilidad y evaluaci√≥n de LangChain
    **Ragas** - Framework de evaluaci√≥n espec√≠fico para sistemas RAG
- **DeepEval** - Framework de evaluaci√≥n de LLM de c√≥digo abierto
- **Langfuse** - Observabilidad de LLM (tier gratuito)
- **Braintrust** - Plataforma de evaluaci√≥n de LLM end-to-end
- **Evals Personalizados** - Escribe tus propios casos de prueba y funciones de
    puntuaci√≥n

### Escenarios de Evaluaci√≥n Comunes

**Escenario 1: Mejora Peque√±a, Mecanismo Sofisticado**
Un equipo muestra una mejora del 8% pero con an√°lisis profundo de errores,
categorizaci√≥n de fallos, y mejoras espec√≠ficas basadas en patrones identificados.
_Evaluaci√≥n:_ Puntos altos en Sofisticaci√≥n del Mecanismo (12-15/15), puntos
medios-altos en Evidencia de Mejora (10-14/20) dependiendo de la consistencia.
**Escenario 2: Mejora Grande, Mecanismo Simple**
Un equipo muestra una mejora del 40% pero el mecanismo es b√°sico: compara
outputs, identifica el peor, lo reemplaza manualmente.
_Evaluaci√≥n:_ Puntos altos en Evidencia de Mejora (15-18/20), puntos bajos-medios
en Sofisticaci√≥n (4-8/15).
**Escenario 3: Enfoque Innovador, Ejecuci√≥n Parcial**
Un equipo tiene una idea brillante para auto-mejora que nadie ha visto, pero solo
funciona en el 60% de los casos.
_Evaluaci√≥n:_ Puntos altos en Creatividad (22-25/25), puntos medios en Funcionalidad
(12-18/25) y Evidencia de Mejora (8-14/20) dependiendo de qu√© tan bien funciona.
**Principio gu√≠a:** Los tres proyectos anteriores podr√≠an competir por el primer lugar
dependiendo de la calidad de ejecuci√≥n y documentaci√≥n. No hay una f√≥rmula √∫nica
para ganar.


### Se√±ales de Alerta (Red Flags)

- **Sin evidencia de mejora:** Afirman que mejor√≥ pero no muestran m√©tricas ni
    ejemplos
- **Mejora manual disfrazada:** El "sistema" es solo cambios manuales entre
    corridas
- **Cherry-picking:** Solo muestran casos donde funcion√≥, ocultan casos donde
    fall√≥
- **Documentaci√≥n inexistente:** C√≥digo sin comentarios, sin README,
    imposible entender qu√© hace
- **Demo no funciona:** Errores cr√≠ticos, no completa el ciclo

## Recursos T√©cnicos

### Acceso a Plataforma OpenAI

Tendr√°s acceso completo a la plataforma OpenAI:

- Documentos de API OpenAI: https://platform.openai.com/docs
- Referencia de API: https://platform.openai.com/docs/api-reference
- Gu√≠a de Ingenier√≠a de Prompts:
    https://platform.openai.com/docs/guides/prompt-engineering

### Frameworks Recomendados

**LangChain / LangGraph** - Altamente recomendado para construir sistemas
complejos de AI:

- Documentos de LangChain:
    https://python.langchain.com/docs/get_started/introduction
- LangGraph para Agentes: https://langchain-ai.github.io/langgraph/
- Excelente para orquestaci√≥n, memoria y cadenas de evaluaci√≥n
**LangChain Academy (Cursos GRATIS - ¬°Altamente Recomendados!):**
- Introducci√≥n a LangGraph:
https://academy.langchain.com/courses/intro-to-langgraph
- Agentes Ambientales con LangGraph:
https://academy.langchain.com/courses/ambient-agents
- Investigaci√≥n Profunda con LangGraph:
https://academy.langchain.com/courses/deep-research-with-langgraph
- Fundamentos de LangSmith: https://academy.langchain.com/

### Papers de Investigaci√≥n 2025

**Lectura Obligatoria:**

**1. Darwin G√∂del Machine: Open-Ended Evolution of Self-Improving Agents
(Mayo 2025)**
    - arXiv: 2505.22954 https://arxiv.org/abs/2505.
    - Paper revolucionario sobre sistemas de AI que reescriben su propio c√≥digo
    - Demuestra mejora de 20% ‚Üí 50% en SWE-bench


- Por qu√© importa: Primer sistema pr√°ctico mostrando verdadera auto-mejora
    mediante modificaci√≥n de c√≥digo
**2. Survey on Evaluation of LLM-based Agents (Marzo 2025)**
- arXiv: 2503.16416 https://arxiv.org/abs/2503.
- Panorama comprensivo de metodolog√≠as de evaluaci√≥n de agentes
- Por qu√© importa: Establece est√°ndares para evaluar sistemas
auto-mejorables
3. **Agentic Context Engineering: Evolving Contexts for Self-Improving
Language Models**
‚óè https://arxiv.org/html/2510.04618v
**Papers Cl√°sicos (A√∫n Relevantes):**
- Self-Refine (2023) - arXiv:2303.17651 https://arxiv.org/abs/2303.
- Reflexion (2023) - arXiv:2303.1 1366 https://arxiv.org/abs/2303.1 1366

### Boilerplate y Configuraci√≥n

**Opciones de Inicio R√°pido:**

- Usa Python con paquetes openai, langchain, y langgraph
- Configura variables de entorno para llaves API
- Crea una prueba simple para verificar acceso a API

## Entregables y M√©todos de Env√≠o

### COMPONENTES REQUERIDOS:

### 1. C√≥digo Fuente (elige el formato m√°s conveniente):

```
‚óè Repositorio GitHub o GitLab
‚óè Link de Replit / CodeSandbox / StackBlitz (c√≥digo ejecutable en l√≠nea)
‚óè Aplicaci√≥n web deployada (Vercel, Netlify, Railway) + link al c√≥digo
```
### 2. Documentaci√≥n (OBLIGATORIA):

Tu proyecto debe incluir un **README.md** (en tu repositorio):
**Contenido m√≠nimo requerido:**
‚óè **Descripci√≥n del problema** que resuelves
‚óè **Arquitectura del sistema** (diagrama opcional pero recomendado)
‚óè **Instrucciones de ejecuci√≥n** (c√≥mo correr tu proyecto)
‚óè **Explicaci√≥n del ciclo de auto-mejora**
‚óè **M√©tricas de mejora:** Evidencia cuantificable (ej: accuracy subi√≥ de 65% a 82%)


### 3. Demostraci√≥n (elige UNO de los siguientes):

**Opci√≥n A - Video Pregrabado:**
‚óè Formato: MP4, MOV, o AVI
‚óè Duraci√≥n: **M√°ximo 5 minutos**
‚óè Contenido: Muestra tu sistema funcionando, el ciclo completo de mejora
‚óè Sube a: **Google Drive, YouTube (no listado), Loom**
‚óè Incluye el link en el formulario de entrega
**Opci√≥n B - Demo en Vivo:**
‚óè Sistema **ejecutable directamente** por los jueces
‚óè Puede ser: Replit ejecutable, app web funcionando, Jupyter Notebook con
instrucciones claras
‚óè Los jueces deben poder ver el ciclo de mejora sin configuraci√≥n compleja
**Opci√≥n C - Screen Recording:**
‚óè Link de **Loom, Google Drive, o plataforma similar**
‚óè M√°ximo 5 minutos
‚óè Narraci√≥n opcional pero recomendada

### 4. Presentaci√≥n (Opcional pero Recomendada):

‚óè M√°ximo **5 slides en PDF**
‚óè √ötil para los finalistas en la ronda final
‚óè Incluye: problema, soluci√≥n, resultados, demostraci√≥n de mejora
**M√âTODO DE ENTREGA:
Primario - Google Form:**
‚óè Link del form: Form
‚óè Campos a completar:
‚óè Nombre del equipo
‚óè Integrantes (nombres y emails)
‚óè Link al c√≥digo (GitHub/Replit/Drive/etc.)
‚óè Link al README o documentaci√≥n
‚óè Link al video/demo
‚óè Link a slides (opcional)
‚óè Comentarios adicionales
**REQUISITOS PARA SER EVALUADO:**
‚óè C√≥digo accesible y ejecutable (o claramente documentado)
‚óè README con las secciones m√≠nimas requeridas
‚óè Demo (video O ejecutable en vivo) que muestre el ciclo de mejora


## Estrategia de Preparaci√≥n

**Para los equipos:**

- Decidan canales de comunicaci√≥n (WhatsApp, Discord, etc.)
- Empiecen a investigar mecanismos de auto-mejora

### Errores Comunes a Evitar

‚ùå **Sobre-Ingenier√≠a
Problema:** Construir un sistema multi-agente complejo con 7 componentes que
nunca funciona end-to-end.
**Soluci√≥n:** Empieza simple. Un agente que mejora sus propios prompts es suficiente
si funciona bien.
‚ùå **M√©tricas Poco Claras
Problema:** "El sistema mejor√≥" sin ning√∫n n√∫mero.
**Soluci√≥n:** Elige 1-2 m√©tricas claras. Rastr√©alas. Muestra la mejora.
‚ùå **Sin Mejora Real
Problema:** Run 2 se desempe√±a igual que Run 1, o peor.
**Soluci√≥n:** Si tu mecanismo de mejora no est√° funcionando, debuggea o simplifica tu
enfoque.

## Reflexiones Finales

### Qu√© Gana Hackathons

```
Demos funcionales - Funcionalidad vence ambici√≥n
Mejora clara - Mu√©stranos los n√∫meros
Buena narrativa - Ay√∫danos a entender por qu√© tu proyecto importa
Gesti√≥n del tiempo - Termina temprano, pule, env√≠a con confianza
Coordinaci√≥n de equipo - Trabajo dividido = progreso m√°s r√°pido
```
### Recuerda

Este es un **sprint, no un marat√≥n**. Tienes tiempo limitado para construir algo
incre√≠ble. Mantenlo simple, mantenlo funcionando, mantenlo medible.

- Entienden el reto profundamente
- Ejecutan eficientemente
- Muestran resultados claros
- Comunican efectivamente


**¬øPreguntas antes del evento?** Email: hackathon@kavak.com
Log√≠stica y Transporte Estacionamiento: Capital Reforma ofrece servicio de valet parking con costo
de $110 MXN por d√≠a. Tambi√©n hay opciones de estacionamiento p√∫blico en las cercan√≠as. Te
recomendamos llegar con tiempo suficiente para encontrar estacionamiento y registrarte antes de las
8:45 AM.

#### ¬°Nos vemos el 23 de octubre! üöÄ

```
Hackathon Kavak x OpenAI 2025
```
