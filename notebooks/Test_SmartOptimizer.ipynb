{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87a4328a",
   "metadata": {},
   "source": [
    "# Test Smart Optimizer - Pruebas de Stubs\n",
    "\n",
    "Este notebook prueba los stubs del Smart Optimizer para asegurar que la infraestructura funciona correctamente.\n",
    "\n",
    "**Nota**: Solo pruebas de stubs pre-evento. No lógica funcional real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504f2496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Agregar src al path\n",
    "sys.path.append(os.path.abspath('src'))\n",
    "\n",
    "# Importar módulos\n",
    "from nodos.evaluar_complejidad import evaluar_complejidad\n",
    "from nodos.generar_refinar import generar_refinar\n",
    "from nodos.validar_calidad import validar_calidad\n",
    "from utils import inicializar_openai\n",
    "from contador import medir_llamada_llm\n",
    "\n",
    "print(\"Imports exitosos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc33bbf",
   "metadata": {},
   "source": [
    "## Prueba de Infraestructura Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da669144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba inicializar_openai\n",
    "cliente = inicializar_openai()\n",
    "print(f\"Cliente OpenAI inicializado: {cliente is not None}\")\n",
    "\n",
    "# Prueba contador en modo demo\n",
    "resultado_demo = medir_llamada_llm(\"Test prompt\", modelo=\"gpt-3.5-turbo\", modo_demo=True)\n",
    "print(f\"Resultado demo: {resultado_demo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ff2eb9",
   "metadata": {},
   "source": [
    "## Prueba de Nodo: Evaluar Complejidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9364e3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba evaluar_complejidad\n",
    "texto_prueba = \"Este es un texto simple para resumir.\"\n",
    "resultado_eval = evaluar_complejidad(texto_prueba)\n",
    "print(f\"Resultado evaluación: {resultado_eval}\")\n",
    "assert isinstance(resultado_eval, dict)\n",
    "assert \"complejidad\" in resultado_eval\n",
    "assert \"modelo\" in resultado_eval\n",
    "print(\"✓ Nodo evaluar_complejidad funciona correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f618d5d0",
   "metadata": {},
   "source": [
    "## Prueba de Nodo: Generar y Refinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e67c0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba generar_refinar\n",
    "prompt_prueba = \"Resume este texto en 3 frases.\"\n",
    "modelo_prueba = \"gpt-3.5-turbo\"\n",
    "resultado_gen = generar_refinar(prompt_prueba, modelo=modelo_prueba)\n",
    "print(f\"Resultado generación: {resultado_gen}\")\n",
    "assert isinstance(resultado_gen, str)\n",
    "print(\"✓ Nodo generar_refinar funciona correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a269867",
   "metadata": {},
   "source": [
    "## Prueba de Nodo: Validar Calidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0759dd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prueba validar_calidad\n",
    "texto_generado = \"Resumen generado.\"\n",
    "texto_original = \"Texto original largo.\"\n",
    "resultado_val = validar_calidad(texto_generado, texto_original)\n",
    "print(f\"Resultado validación: {resultado_val}\")\n",
    "assert isinstance(resultado_val, dict)\n",
    "print(\"✓ Nodo validar_calidad funciona correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900d0ce",
   "metadata": {},
   "source": [
    "## Simulación de Flujo Básico (Sin LangGraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c145573a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simular flujo básico de Smart Optimizer\n",
    "def simular_flujo_smart_optimizer(tarea):\n",
    "    print(f\"--- Iniciando flujo para tarea: {tarea} ---\")\n",
    "    \n",
    "    # 1. Evaluar complejidad\n",
    "    eval_result = evaluar_complejidad(tarea)\n",
    "    print(f\"Evaluación: {eval_result}\")\n",
    "    \n",
    "    # 2. Generar/refinar (usando modelo recomendado)\n",
    "    modelo = eval_result[\"modelo\"]\n",
    "    gen_result = generar_refinar(tarea, modelo=modelo)\n",
    "    print(f\"Generación: {gen_result[:50]}...\")\n",
    "    \n",
    "    # 3. Validar calidad\n",
    "    val_result = validar_calidad(gen_result, tarea)\n",
    "    print(f\"Validación: {val_result}\")\n",
    "    \n",
    "    print(\"--- Flujo completado ---\")\n",
    "    return {\"evaluacion\": eval_result, \"generacion\": gen_result, \"validacion\": val_result}\n",
    "\n",
    "# Ejecutar simulación\n",
    "tarea_ejemplo = \"Resume la historia de México en 100 palabras.\"\n",
    "resultado_flujo = simular_flujo_smart_optimizer(tarea_ejemplo)\n",
    "print(f\"Resultado final: {resultado_flujo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d705a9fd",
   "metadata": {},
   "source": [
    "## Conclusión\n",
    "\n",
    "Todos los stubs funcionan correctamente y retornan tipos esperados. La infraestructura está lista para integrar LangGraph y lógica real durante el evento.\n",
    "\n",
    "**Compliance**: Solo pruebas de stubs, sin violaciones de reglas 5.4/5.5."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
